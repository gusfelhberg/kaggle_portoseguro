{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 variables have too low variance.\n",
      "These variables are ['ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_ind_05_cat_2', 'ps_ind_05_cat_5', 'ps_car_04_cat_3', 'ps_car_04_cat_4', 'ps_car_04_cat_5', 'ps_car_04_cat_6', 'ps_car_04_cat_7', 'ps_car_09_cat_4', 'ps_car_10_cat_1', 'ps_car_10_cat_2']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def drop_columns(dataset,columns_list):\n",
    "    return dataset.drop(columns_list,axis=1)\n",
    "\n",
    "def fill_missing_with_mean(dataset,column):\n",
    "    mean_imp = Imputer(missing_values=-1, strategy='mean', axis=0)\n",
    "    return mean_imp.fit_transform(dataset[[column]]).ravel()\n",
    "\n",
    "def fill_missing_with_most_frequent(dataset,column):\n",
    "    mode_imp = Imputer(missing_values=-1, strategy='most_frequent', axis=0)\n",
    "    return mode_imp.fit_transform(dataset[[column]]).ravel()\n",
    "\n",
    "def get_dummy_variables(dataset,columns_list):\n",
    "    return pd.get_dummies(dataset, columns=columns_list, drop_first=True)\n",
    "\n",
    "def create_interaction_items(dataset,columns_list):\n",
    "    poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "    interactions = pd.DataFrame(data=poly.fit_transform(dataset[columns_list]), columns=poly.get_feature_names(columns_list))\n",
    "    interactions.drop(columns_list, axis=1, inplace=True)  # Remove the original columns\n",
    "\n",
    "    return pd.concat([dataset, interactions], axis=1)\n",
    "\n",
    "\n",
    "train = pd.read_csv('train.csv').drop('id',axis=1)\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "\n",
    "############################################\n",
    "# Balancing of data (the original data has \n",
    "# much more 0 classes then 1 classes)\n",
    "# Source: Kaggle data walkthrough\n",
    "############################################\n",
    "\n",
    "desired_apriori=0.10\n",
    "\n",
    "# Get the indices per target value\n",
    "idx_0 = train[train.target == 0].index\n",
    "idx_1 = train[train.target == 1].index\n",
    "\n",
    "# Get original number of records per target value\n",
    "nb_0 = len(train.loc[idx_0])\n",
    "nb_1 = len(train.loc[idx_1])\n",
    "\n",
    "# Calculate the undersampling rate and resulting number of records with target=0\n",
    "undersampling_rate = ((1-desired_apriori)*nb_1)/(nb_0*desired_apriori)\n",
    "undersampled_nb_0 = int(undersampling_rate*nb_0)\n",
    "# print('Rate to undersample records with target=0: {}'.format(undersampling_rate))\n",
    "# print('Number of records with target=0 after undersampling: {}'.format(undersampled_nb_0))\n",
    "\n",
    "# Randomly select records with target=0 to get at the desired a priori\n",
    "undersampled_idx = shuffle(idx_0, random_state=37, n_samples=undersampled_nb_0)\n",
    "\n",
    "# Construct list with remaining indices\n",
    "idx_list = list(undersampled_idx) + list(idx_1)\n",
    "\n",
    "# Return undersample data frame\n",
    "train = train.loc[idx_list].reset_index(drop=True)\n",
    "\n",
    "\n",
    "############################################\n",
    "\n",
    "# Dropping the variables with too many missing values or huge number of categories\n",
    "vars_to_drop = ['ps_car_01_cat','ps_car_03_cat', 'ps_car_05_cat','ps_car_06_cat','ps_car_11_cat']\n",
    "\n",
    "train = drop_columns(train, vars_to_drop)\n",
    "test = drop_columns(test, vars_to_drop)\n",
    "\n",
    "# Imputing missing values with the mean or mode\n",
    "train['ps_reg_03'] = fill_missing_with_mean(train,'ps_reg_03')\n",
    "train['ps_car_12'] = fill_missing_with_mean(train,'ps_car_12')\n",
    "train['ps_car_14'] = fill_missing_with_mean(train,'ps_car_14')\n",
    "train['ps_car_11'] = fill_missing_with_most_frequent(train,'ps_car_11')\n",
    "\n",
    "test['ps_reg_03'] = fill_missing_with_mean(test,'ps_reg_03')\n",
    "test['ps_car_12'] = fill_missing_with_mean(test,'ps_car_12')\n",
    "test['ps_car_14'] = fill_missing_with_mean(test,'ps_car_14')\n",
    "test['ps_car_11'] = fill_missing_with_most_frequent(test,'ps_car_11')\n",
    "\n",
    "# Features scaling\n",
    "scaler=StandardScaler()\n",
    "continuous=['ps_car_11','ps_car_12','ps_car_13',\n",
    "            'ps_car_14','ps_car_15','ps_calc_01',\n",
    "            'ps_calc_02','ps_calc_03','ps_calc_04','ps_calc_05',\n",
    "            'ps_calc_06','ps_calc_07','ps_calc_08','ps_calc_09',\n",
    "            'ps_calc_10','ps_calc_11','ps_calc_12','ps_calc_13',\n",
    "            'ps_calc_14','ps_reg_01','ps_reg_02','ps_reg_03',\n",
    "            'ps_ind_01','ps_ind_03','ps_ind_14','ps_ind_15']\n",
    "\n",
    "\n",
    "train[continuous] = scaler.fit_transform(train[continuous])\n",
    "test[continuous] = scaler.fit_transform(test[continuous])\n",
    "\n",
    "\n",
    "\n",
    "# Creating dummy variables\n",
    "categorical = ['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat', \n",
    "               'ps_car_02_cat', 'ps_car_04_cat', \n",
    "               'ps_car_07_cat', 'ps_car_08_cat',\n",
    "               'ps_car_09_cat', 'ps_car_10_cat']\n",
    "train = get_dummy_variables(train,categorical)\n",
    "test = get_dummy_variables(test,categorical)\n",
    "\n",
    "# Creating interaction items\n",
    "interaction_items = ['ps_reg_01','ps_reg_02','ps_reg_03','ps_car_12',\n",
    "'ps_car_13','ps_car_14','ps_car_15','ps_calc_01',\n",
    "'ps_calc_02','ps_calc_03']\n",
    "\n",
    "train = create_interaction_items(train,interaction_items)\n",
    "test = create_interaction_items(test,interaction_items)\n",
    "\n",
    "#setting a variance threshold to eliminate variables\n",
    "selector = VarianceThreshold(threshold=.01)\n",
    "selector.fit(train.drop(['target'], axis=1)) # Fit to train without id and target variables\n",
    "selector.fit(test.drop(['id'], axis=1))\n",
    "\n",
    "f = np.vectorize(lambda x : not x) # Function to toggle boolean array elements\n",
    "\n",
    "v = train.drop(['target'], axis=1).columns[f(selector.get_support())]\n",
    "v = test.drop(['id'], axis=1).columns[f(selector.get_support())]\n",
    "print('{} variables have too low variance.'.format(len(v)))\n",
    "print('These variables are {}'.format(list(v)))\n",
    "\n",
    "\n",
    "\n",
    "# Random permutation of train data\n",
    "train = train.sample(frac=1, random_state=1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving files\n",
    "train.to_csv('preprocessed_train.csv',index=None)\n",
    "test.to_csv('preprocessed_test.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
